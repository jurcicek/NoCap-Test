# GPT-2 Performance Bottleneck Summary

## Visual Breakdown: Where Time is Spent

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SINGLE FORWARD PASS TIME BREAKDOWN                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Embedding Layer (2-3%)                                                 â”‚
â”‚  â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  TRANSFORMER BLOCK Ã— 12  (85-90% of total time)            â”‚       â”‚
â”‚  â”‚                                                              â”‚       â”‚
â”‚  â”‚  Attention Layer (45-50% of block time)  ğŸ”´ BOTTLENECK #1  â”‚       â”‚
â”‚  â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚       â”‚
â”‚  â”‚    â€¢ QKV projection: 15%                                    â”‚       â”‚
â”‚  â”‚    â€¢ Attention computation (O(nÂ²)): 60%  â† SLOWEST PART     â”‚       â”‚
â”‚  â”‚    â€¢ Output projection: 15%                                 â”‚       â”‚
â”‚  â”‚    â€¢ RoPE + other: 10%                                      â”‚       â”‚
â”‚  â”‚                                                              â”‚       â”‚
â”‚  â”‚  MLP Layer (35-40% of block time)  ğŸŸ  BOTTLENECK #2        â”‚       â”‚
â”‚  â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚       â”‚
â”‚  â”‚    â€¢ Up projection (768â†’3072): 45%                          â”‚       â”‚
â”‚  â”‚    â€¢ GELU activation: 10%                                   â”‚       â”‚
â”‚  â”‚    â€¢ Down projection (3072â†’768): 45%                        â”‚       â”‚
â”‚  â”‚                                                              â”‚       â”‚
â”‚  â”‚  RMSNorm (5-10% of block time)                             â”‚       â”‚
â”‚  â”‚  â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚       â”‚
â”‚  â”‚                                                              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â”‚  LM Head (5-8%)                                                         â”‚
â”‚  â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Problem: Attention Complexity

### Standard Multi-Head Attention (Current Implementation)

```
For each of 12 layers, for each of 12 heads:

Input (B=4, T=512, C=768)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q, K, V Projections                      â”‚  Cost: O(T Ã— CÂ²)
â”‚  Linear(768 â†’ 768) Ã— 3                    â”‚  ~1.8M ops
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Attention Score Computation  ğŸ”´ SLOW     â”‚  Cost: O(TÂ² Ã— d)
â”‚  Q @ K^T for all token pairs              â”‚  ~134M ops (!)
â”‚  Softmax over T positions                 â”‚
â”‚  Weighted sum with V                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Projection                        â”‚  Cost: O(T Ã— CÂ²)
â”‚  Linear(768 â†’ 768)                        â”‚  ~0.6M ops
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total per head:  ~136M ops
Total per layer: ~1.6B ops (12 heads)
Total model:     ~19B ops (12 layers)
```

**Key insight:** Attention is O(nÂ²) in sequence length!
- For T=512: 262,144 attention scores per head
- For T=1024: 1,048,576 attention scores per head (4Ã— more!)

## Solution #1: Grouped-Query Attention (GQA)

### How GQA Reduces Computation

```
Standard MHA:                    GQA (4 KV heads):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q head 1   â”‚â”€â”€â”€â”€â”€â”           â”‚  Q head 1   â”‚â”€â”€â”€â”€â”€â”
â”‚  K head 1   â”‚     â”‚           â”‚  Q head 2   â”‚     â”‚
â”‚  V head 1   â”‚     â”‚           â”‚  Q head 3   â”‚     â”œâ”€â†’ K head 1
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â†’ Attn    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚   V head 1
â”‚  Q head 2   â”‚     â”‚           â”‚  Q head 4   â”‚â”€â”€â”€â”€â”€â”˜
â”‚  K head 2   â”‚     â”‚           â”‚  Q head 5   â”‚â”€â”€â”€â”€â”€â”
â”‚  V head 2   â”‚     â”‚           â”‚  Q head 6   â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â†’ K head 2
â”‚     ...     â”‚     â”‚           â”‚  Q head 7   â”‚     â”‚   V head 2
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚           â”‚  Q head 8   â”‚â”€â”€â”€â”€â”€â”˜
â”‚  Q head 12  â”‚     â”‚           â”‚  Q head 9   â”‚â”€â”€â”€â”€â”€â”
â”‚  K head 12  â”‚     â”‚           â”‚  Q head 10  â”‚     â”‚
â”‚  V head 12  â”‚â”€â”€â”€â”€â”€â”˜           â”‚  Q head 11  â”‚     â”œâ”€â†’ K head 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  Q head 12  â”‚â”€â”€â”€â”€â”€â”˜   V head 3
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

12 Q projections                12 Q projections
12 K projections  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  4 K projections  (3Ã— fewer!)
12 V projections                4 V projections  (3Ã— fewer!)

Computation: 100%               Computation: ~67% (33% faster!)
```

**Savings:**
- K/V projection: 3Ã— faster (4 instead of 12)
- K/V memory: 3Ã— less
- Attention quality: ~99% retained

## Solution #2: Reduce MLP Expansion

### Current vs Optimized MLP

```
Current MLP (4Ã— expansion):         Optimized MLP (3Ã— expansion):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  768 dim      â”‚                   â”‚  768 dim      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â†“ Linear                            â†“ Linear
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3072 dim     â”‚  2.36M params     â”‚  2304 dim     â”‚  1.77M params
â”‚               â”‚                   â”‚               â”‚
â”‚  GELU         â”‚                   â”‚  GELU         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â†“ Linear                            â†“ Linear
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  768 dim      â”‚                   â”‚  768 dim      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Parameters: 2.36M                   Parameters: 1.77M (25% fewer!)
Time: 100%                          Time: ~75% (25% faster!)
```

## Combined Impact: Attention Optimization Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ATTENTION MECHANISM COMPARISON                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mechanism      â”‚ Q Heads  â”‚  KV Heads    â”‚   Speedup    â”‚  Quality  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Standard MHA   â”‚    12    â”‚     12       â”‚   1.00Ã—      â”‚   100%    â”‚
â”‚ GQA (4 groups) â”‚    12    â”‚      4       â”‚   1.25Ã—      â”‚   99.5%   â”‚
â”‚ GQA (2 groups) â”‚    12    â”‚      2       â”‚   1.40Ã—      â”‚   99.0%   â”‚
â”‚ MQA (extreme)  â”‚    12    â”‚      1       â”‚   1.50Ã—      â”‚   98.0%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Memory Usage Breakdown

```
For B=4, T=512, n_heads=12, n_embd=768, n_layers=12:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL PARAMETERS                                           â”‚
â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  ~500 MB               â”‚
â”‚                                                             â”‚
â”‚  OPTIMIZER STATE (AdamW)                                    â”‚
â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  ~1000 MB              â”‚
â”‚  (2Ã— model size for momentum + variance)                   â”‚
â”‚                                                             â”‚
â”‚  ACTIVATIONS (forward pass)  ğŸ”´ MAJOR MEMORY USER          â”‚
â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  ~2000 MB      â”‚
â”‚    â€¢ Attention matrices: ~70 MB per layer                  â”‚
â”‚    â€¢ MLP activations: ~40 MB per layer                     â”‚
â”‚    â€¢ Residual streams: ~12 MB per layer                    â”‚
â”‚                                                             â”‚
â”‚  GRADIENTS (backward pass)                                 â”‚
â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  ~500 MB               â”‚
â”‚                                                             â”‚
â”‚  TOTAL:  ~4 GB                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With Flash Attention:
â”‚  ACTIVATIONS (forward pass)  â† REDUCED!                    â”‚
â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~600 MB          â”‚
â”‚  (Don't materialize full attention matrices)               â”‚
â”‚                                                             â”‚
â”‚  TOTAL:  ~2.5 GB (38% less memory!)                        â”‚
```

## Sequence Length Impact

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ATTENTION COST vs SEQUENCE LENGTH (O(nÂ²))              â”‚
â”‚                                                                 â”‚
â”‚  Cost                                                           â”‚
â”‚    ^                                                            â”‚
â”‚    â”‚                                                       â—    â”‚ T=1024
â”‚    â”‚                                                            â”‚
â”‚    â”‚                                            â—               â”‚ T=512
â”‚    â”‚                                                            â”‚
â”‚    â”‚                              â—                             â”‚ T=256
â”‚    â”‚                                                            â”‚
â”‚    â”‚                  â—                                         â”‚ T=128
â”‚    â”‚                                                            â”‚
â”‚    â”‚         â—                                                  â”‚ T=64
â”‚    â”‚                                                            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚
â”‚             Sequence Length (T)                                â”‚
â”‚                                                                 â”‚
â”‚  Quadratic growth means 2Ã— longer sequences = 4Ã— more compute! â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Solutions:
  â€¢ Sliding Window: O(n Ã— window) - linear in sequence length
  â€¢ Flash Attention: Same O(nÂ²) but much faster constant factors
  â€¢ GQA/MQA: Reduces coefficient of O(nÂ²) term
```

## Quick Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  What optimization should I implement first?        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Sequence length? â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
    T < 512           T â‰¥ 512
         â”‚                 â”‚
         â†“                 â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Implement:  â”‚   â”‚   Implement:     â”‚
  â”‚  1. GQA      â”‚   â”‚   1. Sliding     â”‚
  â”‚  2. 3Ã— MLP   â”‚   â”‚      Window      â”‚
  â”‚  3. Flash    â”‚   â”‚   2. Flash Attn  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   3. GQA         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     
     Both paths should also add:
     â€¢ SwiGLU (faster learning)
     â€¢ Better optimizer
```

## Expected Results

### After Implementing GQA + Flash + 3Ã— MLP:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric           â”‚  Before   â”‚  After   â”‚  Improvement â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tokens/second    â”‚   5,000   â”‚  7,700   â”‚    +54%      â”‚
â”‚ Time per step    â”‚   51 ms   â”‚  33 ms   â”‚    -35%      â”‚
â”‚ Peak memory      â”‚  8.5 GB   â”‚  6.0 GB  â”‚    -29%      â”‚
â”‚ Val loss @ 1000  â”‚   3.50    â”‚  3.48    â”‚   +0.6%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Action Items

1. âœ… Read `OPTIMIZATION_GUIDE.md` for detailed instructions
2. âœ… Run `python profile_model.py` to see your bottlenecks
3. âœ… Implement GQA (see `gqa_implementation.py`)
4. âœ… Test and measure speedup
5. âœ… Add Flash Attention to standard attention
6. âœ… Reduce MLP expansion to 3Ã—
7. âœ… Experiment with SwiGLU

**Expected time: 1-2 hours**
**Expected speedup: 1.5-2.0Ã— faster training**


