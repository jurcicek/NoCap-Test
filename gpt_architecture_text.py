#!/usr/bin/env python3
"""
Text-based GPT Architecture Visualization
Creates a clear text representation of the GPT model architecture
"""

def print_gpt_architecture():
    """Print a detailed text-based architecture diagram"""
    
    print("=" * 80)
    print("GPT-2 ARCHITECTURE (PyTorch Implementation)")
    print("=" * 80)
    print()
    
    print("INPUT PROCESSING:")
    print("┌─────────────────────────────────────────────────────────────┐")
    print("│ Input Tokens (B, T) → Token Embedding (vocab_size → n_embd) │")
    print("└─────────────────────────────────────────────────────────────┘")
    print("                           ↓")
    print()
    
    print("TRANSFORMER BLOCKS (n_layer blocks, each containing):")
    print("┌─────────────────────────────────────────────────────────────┐")
    print("│                    TRANSFORMER BLOCK                        │")
    print("│  ┌─────────────────────────────────────────────────────┐    │")
    print("│  │ 1. RMSNorm(x)                                      │    │")
    print("│  │ 2. Causal Self-Attention with RoPE                 │    │")
    print("│  │    ┌─────────────────────────────────────────────┐  │    │")
    print("│  │    │ • QKV Projection: Linear(n_embd → 3*n_embd) │  │    │")
    print("│  │    │ • Apply Rotary Position Embeddings to Q,K   │  │    │")
    print("│  │    │ • Scaled Dot-Product Attention (causal mask) │  │    │")
    print("│  │    │ • Output Projection: Linear(n_embd → n_embd)  │  │    │")
    print("│  │    └─────────────────────────────────────────────┘  │    │")
    print("│  │ 3. Residual Connection: x = x + attention(x)       │    │")
    print("│  │ 4. RMSNorm(x)                                      │    │")
    print("│  │ 5. MLP:                                            │    │")
    print("│  │    ┌─────────────────────────────────────────────┐  │    │")
    print("│  │    │ • Linear(n_embd → 4*n_embd)                 │  │    │")
    print("│  │    │ • GELU Activation                            │  │    │")
    print("│  │    │ • Linear(4*n_embd → n_embd)                 │  │    │")
    print("│  │    └─────────────────────────────────────────────┘  │    │")
    print("│  │ 6. Residual Connection: x = x + mlp(x)           │    │")
    print("│  └─────────────────────────────────────────────────────┘    │")
    print("└─────────────────────────────────────────────────────────────┘")
    print("                           ↓")
    print()
    
    print("OUTPUT PROCESSING:")
    print("┌─────────────────────────────────────────────────────────────┐")
    print("│ Final RMSNorm(x) → LM Head (n_embd → vocab_size) → Logits  │")
    print("└─────────────────────────────────────────────────────────────┘")
    print()
    
    print("MODEL CONFIGURATIONS:")
    print("┌─────────┬─────────┬─────────┬─────────┬─────────────────────┐")
    print("│ Model   │ Layers  │ Heads   │ Dims    │ Parameters          │")
    print("├─────────┼─────────┼─────────┼─────────┼─────────────────────┤")
    print("│ d12     │   12    │   12    │  768    │ ~124M              │")
    print("│ d24     │   24    │   16    │ 1024    │ ~355M              │")
    print("│ d36     │   36    │   20    │ 1280    │ ~774M              │")
    print("│ d48     │   48    │   25    │ 1600    │ ~1.5B              │")
    print("└─────────┴─────────┴─────────┴─────────┴─────────────────────┘")
    print()
    
    print("KEY FEATURES:")
    print("• Rotary Position Embeddings (RoPE) - applied to Q and K")
    print("• RMSNorm - root mean square normalization (instead of LayerNorm)")
    print("• Weight Tying - embedding weights shared with output layer")
    print("• Causal Self-Attention - prevents future token access")
    print("• GELU Activation - in MLP layers")
    print("• Residual Connections - around attention and MLP")
    print()
    
    print("ATTENTION MECHANISM DETAILS:")
    print("┌─────────────────────────────────────────────────────────────┐")
    print("│ 1. Input (B, T, C) → QKV Projection → Q, K, V (B, T, C)   │")
    print("│ 2. Reshape to (B, T, n_head, head_dim)                    │")
    print("│ 3. Apply RoPE to Q and K                                  │")
    print("│ 4. Transpose to (B, n_head, T, head_dim)                 │")
    print("│ 5. Scaled Dot-Product Attention:                         │")
    print("│    Attention(Q,K,V) = softmax(QK^T/√d_k + M)V            │")
    print("│    where M is causal mask (upper triangular -∞)           │")
    print("│ 6. Output projection and reshape back to (B, T, C)        │")
    print("└─────────────────────────────────────────────────────────────┘")
    print()
    
    print("TRAINING FEATURES:")
    print("• Distributed Data Parallel (DDP) training")
    print("• Gradient accumulation for large effective batch sizes")
    print("• Mixed precision training (bfloat16)")
    print("• Model compilation with torch.compile()")
    print("• Custom data loader for binary token files")
    print("• Weights & Biases logging integration")
    print()
    
    print("DATA FLOW:")
    print("Input Tokens → Embedding → [Transformer Blocks] → RMSNorm → LM Head → Logits")
    print("     ↓              ↓              ↓                    ↓         ↓")
    print("  (B,T)        (B,T,C)       (B,T,C)              (B,T,C)   (B,T,vocab)")
    print()
    print("Where: B=batch_size, T=sequence_length, C=n_embd, vocab=vocab_size")

if __name__ == "__main__":
    print_gpt_architecture()
